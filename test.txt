import time
import pandas as pd
import numpy as np
from pyspark.sql import SparkSession
from my_spark import SparkSessionManager

# Create synthetic datasets
def create_csv(file_path, num_rows):
    df = pd.DataFrame({
        'id': np.arange(num_rows),
        'value1': np.random.rand(num_rows),
        'value2': np.random.rand(num_rows),
        'category': np.random.choice(['A', 'B', 'C', 'D'], num_rows)
    })
    df.to_csv(file_path, index=False)

# Pandas operations
def pandas_operations(file_path):
    start_time = time.time()
    df = pd.read_csv(file_path)
    read_time = time.time() - start_time
    
    start_time = time.time()
    df['value_sum'] = df['value1'] + df['value2']
    add_time = time.time() - start_time
    
    start_time = time.time()
    result = df.groupby('category').agg({'value_sum': 'sum'}).reset_index()
    groupby_time = time.time() - start_time
    
    return read_time, add_time, groupby_time

# Spark operations
def spark_operations(file_path):
    with SparkSessionManager(app_name="PerformanceComparison") as spark:
        start_time = time.time()
        df = spark.read.csv(file_path, header=True, inferSchema=True)
        read_time = time.time() - start_time
        
        start_time = time.time()
        df = df.withColumn('value_sum', df['value1'] + df['value2'])
        add_time = time.time() - start_time
        
        start_time = time.time()
        result = df.groupBy('category').sum('value_sum').toPandas()
        groupby_time = time.time() - start_time
        
        return read_time, add_time, groupby_time

def main():
    file_sizes = [10**4, 10**5, 10**6, 10**7]
    
    for size in file_sizes:
        file_path = f"dataset_{size}.csv"
        create_csv(file_path, size)
        
        print(f"\n--- Dataset Size: {size} ---")
        
        print("\nPandas Performance")
        pandas_times = pandas_operations(file_path)
        print(f"Pandas times (read, add, groupby): {pandas_times}")
        
        print("\nSpark Performance")
        spark_times = spark_operations(file_path)
        print(f"Spark times (read, add, groupby): {spark_times}")

if __name__ == '__main__':
    main()