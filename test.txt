import pandas as pd
import numpy as np
import time
from pyspark.sql import SparkSession
from my_spark import SparkSessionManager

# Create a large synthetic dataset with random data
def create_large_csv(file_path, num_rows=10**7):
    print("Creating large CSV file...")
    df = pd.DataFrame({
        'id': np.arange(num_rows),
        'value1': np.random.rand(num_rows),
        'value2': np.random.rand(num_rows),
        'category': np.random.choice(['A', 'B', 'C', 'D'], num_rows)
    })
    df.to_csv(file_path, index=False)
    print("CSV file created.")

# Pandas operations
def pandas_operations(file_path):
    start_time = time.time()
    df = pd.read_csv(file_path)
    print(f"Pandas: Read CSV in {time.time() - start_time:.2f} seconds")
    
    start_time = time.time()
    df['value_sum'] = df['value1'] + df['value2']
    print(f"Pandas: Column addition in {time.time() - start_time:.2f} seconds")
    
    start_time = time.time()
    result = df.groupby('category').agg({'value_sum': 'sum'}).reset_index()
    print(f"Pandas: Group by and aggregation in {time.time() - start_time:.2f} seconds")
    
    return result

# Spark operations
def spark_operations(file_path):
    start_time = time.time()
    with SparkSessionManager(app_name="PerformanceComparison") as spark:
        df = spark.read.csv(file_path, header=True, inferSchema=True)
        print(f"Spark: Read CSV in {time.time() - start_time:.2f} seconds")
        
        start_time = time.time()
        df = df.withColumn('value_sum', df['value1'] + df['value2'])
        print(f"Spark: Column addition in {time.time() - start_time:.2f} seconds")
        
        start_time = time.time()
        result = df.groupBy('category').sum('value_sum').toPandas()
        print(f"Spark: Group by and aggregation in {time.time() - start_time:.2f} seconds")
        
        return result

def main():
    file_path = "large_dataset.csv"
    
    # Create a large CSV file if it doesn't exist
    create_large_csv(file_path)
    
    print("\n--- Pandas Performance ---")
    pandas_result = pandas_operations(file_path)
    
    print("\n--- Spark Performance ---")
    spark_result = spark_operations(file_path)
    
    print("\n--- Results ---")
    print("Pandas Result:")
    print(pandas_result)
    print("\nSpark Result:")
    print(spark_result)

if __name__ == '__main__':
    main()
